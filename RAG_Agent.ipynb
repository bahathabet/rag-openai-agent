{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "import gradio as gr\n",
    "from openai import AsyncOpenAI\n",
    "import numpy as np\n",
    "from agents import Agent, Runner, trace, function_tool, OpenAIChatCompletionsModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to use models with api keys (openai, deepseek, anthropic) otherwise just comment them\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Explicit variable assignments\n",
    "openai_api_key     = os.getenv('OPENAI_API_KEY')\n",
    "google_api_key     = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key   = os.getenv('DEEPSEEK_API_KEY')\n",
    "anthropic_api_key  = os.getenv('ANTHROPIC_API_KEY')\n",
    "\n",
    "# List of variable names (as strings)\n",
    "var_names = [\n",
    "    \"openai_api_key\", \"google_api_key\", \"deepseek_api_key\", \"anthropic_api_key\"\n",
    "            ]\n",
    "\n",
    "\n",
    "for var in var_names:\n",
    "    value = locals()[var]\n",
    "    label = var.title()\n",
    "    if value:\n",
    "        print(f\"{label} exists and begins {value[:4]}\")\n",
    "    else:\n",
    "        print(f\"{label} not set\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I used the following prompt in Claude to generate text related to A FAKE company called ParmaTIS!\n",
    " Generate 4 text files, each contains 600 words for a company called Parma Tecno Intelligent Services (ParmaTIS). I will use these generated data in some RAG and LLMs examples to simulate an existing company. the company located in Parma- Italy, established in 2018 to offer AI services. the first file generate text related to ABOUT the company including and not limited to mission, vision, value, history and objectives. the second file, generate text related to the 5 departments and 15 staff members in the company, provide Italian names for the staff, their positions and roles and a bio for each person, the CEO should be a Computer Engineer and graduated from the Universita di Parma . the third file generate text related to the 6 services of the company such that it offers Energy solutions,  Agentic AI solutions, Business intelligence solutions, cyber security and so on. the forth file generate text related to the previous and current projects portfolio, name and describe 5 previous done projects and 3 current projects they are working on. In each file generate texts in paragraphs seperated by new lines. generate the text in professional way to simulate an existing running company. export the files as name.txt. It is important to generate coherent text and do not repeat text in each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads all .txt documents recursively from the knowledge-base3/ directory using DirectoryLoader.\n",
    "# Each document is enriched with a doc_type metadata field based on its filename (excluding extension)\n",
    "documents = []\n",
    "folders = glob.glob(\"knowledge-base4/\")\n",
    "encoding_loader = {'encoding': 'utf-8'}\n",
    "# encoding_loader={'autodetect_encoding': True}\n",
    "for folder in folders:  \n",
    "    docs_loader = DirectoryLoader(folder, glob=\"**/*.txt\", \n",
    "                             loader_cls=TextLoader, \n",
    "                             loader_kwargs=encoding_loader)\n",
    "    docs_folder = docs_loader.load()\n",
    "    for doc in docs_folder:\n",
    "        file_name = os.path.basename(doc.metadata[\"source\"])\n",
    "        doc.metadata[\"page_title\"] = os.path.splitext(file_name)[0]\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(separator=\".\", chunk_size=8000, chunk_overlap=500)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A text embedding is a numerical representation of a text, usually as a dense vector of real numbers. \n",
    "# It captures semantic meaning in a format models can understand and compare.\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and delete the previous chroma datastore\n",
    "ds_name = \"vector_store\"\n",
    "if os.path.exists(ds_name):\n",
    "    Chroma(persist_directory=ds_name, embedding_function=embeddings).delete_collection()\n",
    "\n",
    "# Create the vector datastore with chunk embedding\n",
    "vector_store = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=ds_name)\n",
    "print(f\"{vector_store._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To read data from a previous vector store directory uncomment the following:\n",
    "# vector_store = Chroma(persist_directory=ds_name, embedding_function=embeddings)\n",
    "ds = vector_store.get(include=['embeddings', \"documents\", \"metadatas\"])\n",
    "vectors = np.array(ds['embeddings'])\n",
    "docs = ds[\"documents\"]\n",
    "metadatas = ds[\"metadatas\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to perform search\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "results = retriever.invoke(\"What are the latest trends in Parma tis?\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what are the active projects?\"\n",
    "results = vector_store.similarity_search(query, k=3)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the RAG chain agent with OpenAI / Ollama(locally)\n",
    "RAG_agent1 = ChatOpenAI(temperature=0.7, model_name=\"gpt-4o-mini\")\n",
    "RAG_agent2 = ChatOllama(model=\"llama3.2:latest\")\n",
    "\n",
    "# Return VectorStoreRetriever initialized from this VectorStore.\n",
    "retriever = vector_store.as_retriever()\n",
    "# This stores the entire conversation history in memory without any additional processing.\n",
    "RAG_memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "# Setup the conversation chain with the LLM, vectorstore doc retriever and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=RAG_agent1, retriever=retriever, memory=RAG_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing RAG agent\n",
    "query = \"what is parma tis?\"\n",
    "result = conversation_chain.invoke({\"question\":query})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def vectorstore_retriever_tool(question: str) -> str:\n",
    "    \"\"\"Use this tool to answer any question that might be related to the content of uploaded documents, \\\n",
    "        Parma Tecno Intelligent Services (ParmaTIS) parma tis company knowledge base, or reference material. \\\n",
    "        If the user is asking about people, dates, procedures,\\\n",
    "         services, working areas, projects or named entities, use this tool.\"\"\"\n",
    "    return conversation_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_text = \"\"\"You are an assistant that answers user questions. You can do two things:\n",
    "\n",
    "1. If the user's question can be answered from general knowledge, answer directly. Example: Q: What is Calculas? → general knowledge\n",
    "2. If the user asks about anything related to the uploaded documents in the RAG vectorstore, people names,\\\n",
    "   company names like (Parma Tecno Intelligent Services, ParmaTIS, parma TIS, parma tis, parmatis) dates, \\\n",
    "   processes, projects, services, staff, roles, locations, departments or data, use the `vectorstore_retriever_tool` tool to get an accurate answer.\\\n",
    "   This tool has access to private Parma Tecno Intelligent Services, ParmaTIS company documents and RAG-embedded content.\\\n",
    "   You should start the answer with 'RAG Agent Response:'. \\\n",
    "   Always choose to call the tool if you're unsure whether you have enough context to answer from general knowledge.\\\n",
    "   If you do not know the answer simply say I do not know, do not generate irrelevant text.  Example: Q: Who is Marco Rossi? → use RAG tool  \n",
    "   \n",
    "## If you used the `vectorstore_retriever_tool` tool in answering the user question, then You should start saying \"RAG Agent Response:\" \\\n",
    "   then continue with the answer.\n",
    "## Always prefer calling the tool if you're unsure whether you have enough context to answer from general knowledge.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate different LLM models to be used used with the orchestrator OpenAI SDK Agent\n",
    "GEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "DEEPSEEK_BASE_URL = \"https://api.deepseek.com/v1\"\n",
    "CLAUDE_BASE_URL = \"https://api.anthropic.com/v1/\"\n",
    "\n",
    "claude_client = AsyncOpenAI(base_url=CLAUDE_BASE_URL, api_key=anthropic_api_key)\n",
    "deepseek_client = AsyncOpenAI(base_url=DEEPSEEK_BASE_URL, api_key=deepseek_api_key)\n",
    "lama_client = AsyncOpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "# gemini_client = AsyncOpenAI(base_url=GEMINI_BASE_URL, api_key=google_api_key)\n",
    "\n",
    "deepseek_model = OpenAIChatCompletionsModel(model=\"deepseek-chat\", openai_client=deepseek_client)\n",
    "claude_model=OpenAIChatCompletionsModel(model=\"claude-3-7-sonnet-20250219\", openai_client=claude_client)\n",
    "llama_model = OpenAIChatCompletionsModel(model=\"llama3.2:latest\", openai_client=lama_client)\n",
    "# Gemini model does not work with OpenAi Agents SDK\n",
    "# gemini_model = OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\", openai_client=gemini_client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the orchestrator OpenAI SDK Agents, anyone can be used\n",
    "\n",
    "orch_agent_llama = Agent(name=\"llama_agent\", instructions=sys_text, model=llama_model, tools=[vectorstore_retriever_tool])\n",
    "orch_agent_claude = Agent(name=\"claude_agent\", instructions=sys_text, model=claude_model, tools=[vectorstore_retriever_tool])\n",
    "orch_agent_dseek = Agent(name=\"deepseek_agent: deepseek_model\", model=deepseek_model, instructions=sys_text, tools=[vectorstore_retriever_tool])\n",
    "# for openai models, you need to define the model name in the model parameter, no need to define the client and model\n",
    "orch_agent_gpt = Agent(name=\"chat_agent: gpt-4o-mini\", model=\"gpt-4o-mini\", instructions=sys_text, tools=[vectorstore_retriever_tool])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def async_agent_call(input_list: list):\n",
    "\n",
    "     with trace(\"ParmaTIS_Agent\"):\n",
    "        result = await Runner.run(orch_agent_gpt, input_list)\n",
    "        \n",
    "        return result.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the orchestrator agent\n",
    "prompt = [{\"role\": \"user\", \"content\": \"who is parma tis?\"}]\n",
    "await async_agent_call(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Management\n",
    "In this architecture, there are three types of memories:\n",
    "\n",
    "1. RAG_memory: a langchain memory, visible for the RAG_Agent only.\n",
    "2. chat_memory: combines the RAG_memory and the the orchestrator agent memory, visible for the orchestrator agent.\n",
    "3. history_display: same with chat_memory, used for the chat UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_memory.clear() # clear the langchain memory from previous chatbot run\n",
    "with gr.Blocks(theme=gr.themes.Soft(font=[gr.themes.GoogleFont(\"Inconsolata\"), \"Arial\", \"sans-serif\"])) as demo:\n",
    "    gr.Markdown(\"## 🤖 ParmaTIS AI Assistant\")\n",
    "    gr.Markdown(\n",
    "        \"Ask me about ParmaTIS company, employees, activities and services, or any general question to answer precisely.\"\n",
    "    )\n",
    "\n",
    "    chatbot = gr.Chatbot(\n",
    "        label=\"Assistant\",\n",
    "        show_copy_button=True,\n",
    "        bubble_full_width=False,\n",
    "        layout=\"bubble\",  # bubble or panel\n",
    "        height=500\n",
    "    )\n",
    "\n",
    "    chat_memory = gr.State([])  # session-specific memory\n",
    "\n",
    "    with gr.Row():\n",
    "        txt = gr.Textbox(\n",
    "            show_label=False,\n",
    "            placeholder=\"Type your message and hit Enter...\",\n",
    "            lines=1,\n",
    "            scale=8,\n",
    "            autofocus=True,\n",
    "            container=False\n",
    "        )\n",
    "        submit_btn = gr.Button(\"🚀 Send\", scale=1)\n",
    "\n",
    "    with gr.Row():\n",
    "        clear_btn = gr.Button(\"🧹 Clear Chat\")\n",
    "        # Add more buttons here like regenerate, save, etc.\n",
    "\n",
    "    async def respond(message, history_display, chat_memory):\n",
    "        # Append user message\n",
    "        chat_memory.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "        # Show temporary bot response while processing\n",
    "        history_display.append((message, \"⏳ Thinking...\"))\n",
    "        yield \"\", history_display, chat_memory\n",
    "\n",
    "        # Call your async agent\n",
    "        response = await async_agent_call(chat_memory)\n",
    "\n",
    "        # Replace placeholder with real response\n",
    "        history_display[-1] = (message, response)\n",
    "        chat_memory.append({\"role\": \"assistant\", \"content\": response})\n",
    "        yield \"\", history_display, chat_memory\n",
    "\n",
    "    def clear():\n",
    "        RAG_memory.clear()\n",
    "        return \"\", [], []\n",
    "\n",
    "    # Bind submit via textbox and button\n",
    "    txt.submit(respond, [txt, chatbot, chat_memory], [txt, chatbot, chat_memory])\n",
    "    submit_btn.click(respond, [txt, chatbot, chat_memory], [txt, chatbot, chat_memory])\n",
    "\n",
    "    clear_btn.click(clear, outputs=[txt, chatbot, chat_memory])\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
